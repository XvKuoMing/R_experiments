---
title: "HW2. Hypothesis testing"
output: html_document
---

# HW2. Тестирование гипотез. t-test. Доверительные интервалы. Корреляции 

Перед сдачей домашнего задания рекомендуем запустить Run All или сгенерировать html- или pdf-страницу с помощью Knit, чтобы убедиться, что в финальной версии весь ваш код будет запускаться без проблем.

Файл Rmd вы можете а) положить в свой приватный репозиторий da4clcourse на GitHub (в корне репозитория под именем HW1.Rmd). Дайте доступ преподавателю и ассистенту к приватному репозиторию (пригласите @olesar и @The-One-Who-Speaks-and-Depicts) или б) выслать преподавателю на почту olyashevskaya@hse.ru с темой da4clcourse.

## 1. Частотность и фонетика

В этом разделе используются те же данные, что и в прошлом домашнем задании. Вы можете пропустить описание, если знакомы с датасетом.  

Во многих лингвистических исследованиях отмечается, что часто используемые в языке слова звучат короче, а при их произнесении наблюдается редукция и коартикуляция. Работа Fabian Tomaschek et al. (2018) исследует гипотезу, что моторные навыки произнесения улучшаются с опытом, который, в свою очередь, напрямую связан с частотностью слова. Ученые попросили испытуемых (17 бакалавров университета Тюбингена, 8 мужчин и 9 женщин) прочитать вслух немецкие глаголы, содержащие звук [a:] в основе. Испытуемые были поставлены в экспериментальные условия, которые исподволь заставляли их читать быстрее или медленнее (slow/fast condition).

В этом задании мы просим вас сравнить длину звучания всего слова целиком, а также длину звучания интересующего ученых сегмента (звука [a:]) в условиях slow и fast. Хотя логично предположить, что в условии fast произнесение и слов, и сегментов будет короче, все же нужно убедиться, что данные это подтверждают, прежде чем переходить к более сложному анализу по сути вопроса. Кроме того, мы будем уверены, что экспериментальные условия были должным образом соблюдены, ученые не запутались в кодировании данных и документировали результаты корректно.

Интересующие нас переменные:

LogDurationW - log-transformed word duration (логарифм длины произнесения слова)
LogDurationA - log-transformed segment duration (логарифм длины произнесения сегмента)
Cond - condition: slow vs. fast (условие).

Ссылка на данные [link](https://raw.githubusercontent.com/LingData2019/LingData2020/master/data/dur_word_frequency.csv)

### 1.0 t-test

С помощью t-критерия Стьюдента мы хотим определить статистическую значимость различия значений применительно к следующим переменным:  

(a) word durarion in fast condition and word duration in slow condition,

(b) segment duration in fast condition and segment duration in slow condition. 

Другими словами, мы хотим проверить, что различие этих длительностей имеет место не только на выборках, но и в генеральной совокупности.

#### 1.1 Гипотезы 

В первую очередь, сформулируйте нулевую гипотезу (H_0) и альтернативную гипотезу (H_1). Если для выполнения задания нужно сформулировать несколько H_0 и H_1, сделайте это, продублировав этот и следующий разделы (1.1а, 1.2a, 1.3.a, 1.1b, 1.2b и т.д.)  

```
a)

H0: ДЛИНА ЗВУЧАНИЯ СЛОВА при быстром прочтении НЕ ОТЛИЧАЕТСЯ от медленного прочтения

H1: ДЛИНА ЗВУЧАНИЯ СЛОВА при быстро прочтении КОРОЧЕ (отличается в меньшую сторону), чем при медленном прочтении

b) Нам также необходимо сформулировать гипотезы для b

HO: ДЛИНА ЗВУЧАНИЯ СЕГМЕНТА при быстром прочтении НЕ ОТЛИЧАЕТСЯ от медленного прочтения

H1: ДЛИНА ЗВУЧАНИЯ СЕГМЕНТА при быстро прочтении КОРОЧЕ (отличается в меньшую сторону), чем при медленном прочтении 

```

#### 1.2 Тест  

Загрузите данные и примените `t.test` для проверки гипотез.  

```{r}
library(tidyverse)
library(skimr)
```


```{r t-test}
dur_word_freq <- read_csv("https://raw.githubusercontent.com/LingData2019/LingData2020/master/data/dur_word_frequency.csv")
dur_word_freq
```

##### 1.2 ТЕСТ a


```{r t-test a}
t.test(dur_word_freq$LogDurationW ~ dur_word_freq$Cond, alternative='less', paired=FALSE)
```

```{r t-test a visualized}
ggstatsplot::ggbetweenstats(data = dur_word_freq, x = Cond, y = LogDurationW,
  effsize.type = "d", mean.ci = TRUE,
  pairwise.comparisons = FALSE, 
  messages = TRUE, bf.message = FALSE,
  title = "t-test для двух независимых выборок", xlab = "Условия", ylab = "Логарифм длины звучания слова")
```





##### 1.2 ТЕСТ b

```{r t-test b}
t.test(dur_word_freq$LogDurationA ~ dur_word_freq$Cond, alternative='less', paired=FALSE)
```


```{r t-test b visualized}
ggstatsplot::ggbetweenstats(data = dur_word_freq, x = Cond, y = LogDurationA,
  effsize.type = "d", mean.ci = TRUE,
  pairwise.comparisons = FALSE, 
  messages = TRUE, bf.message = FALSE,
  title = "t-test для двух независимых выборок", xlab = "Условия", ylab = "Логарифм длины звучания сегмента")
```




#### 1.3 Интерпретация  

Интерпретируйте результаты t-теста. Включите в вывод t-статистику, степени свободы и p-values. Можно ли заключить, что имеется различие в длительности слов в быстрых и медленных условиях в генеральной совокупности? Можно ли заключить то же самое в отношении длительности сегментов?  

```
В обоих случаях p-value < 0.05, это позволяет нам отвергнуть гипотезу о том, что длительность слов и сегментов одинаково в обоих условиях
```

### 2. Дискуссия 

Возможно, применение t-test в чистом виде является не лучшей опцией для решения задачи 1.0. Если вам тоже так кажется, приведите аргументы против его использования и предложите другое решение. 

```
Как мы видим из графиков и значения p-value (оно сильно меньше 0.05), данные не нормально распределены. Это нарушает главное допущение t-test'a. Поэтому вместо него нам нужно использовать непараметрический аналог.

```

#### 2.1 Код для решения    

Напишите код, который может помочь в аргументации и покажет другое решение. 

```{r}
shapiro.test(dur_word_freq$LogDurationW)
```

```{r}
shapiro.test(dur_word_freq$LogDurationA)
```



```{r wilcox-test-a}
wilcox.test(LogDurationW ~ Cond,
            data=dur_word_freq,
            alternative="less",
            paired=FALSE)
```


```{r wilcox-test-b}
wilcox.test(LogDurationA ~ Cond,
            data=dur_word_freq,
            alternative="less",
            paired=FALSE) # наши новые результаты также отвергают нулевую гипотезу
```




### 3. Доверительный интервал

#### 3.1 Формула в явном виде 

Ниже приведена формула для вычисления 95% доверительного интервала:

$$
\mathrm{CI} = \left[\bar{x} - 1.96\times \frac{\mathop{\mathrm{sd}}(x)}{\sqrt{n}},\ \bar{x} + 1.96\times \frac{\mathop{\mathrm{sd}}(x)}{\sqrt{n}}\right].
$$
Используйте ее для нахождения 95% доверительного интервала для популяционной средней длительности слова (средней в генеральной совокупности).  


```{r ci-formula}

ci_ <- function(sample_mean, sample_sd, sample_size){
  sd_sqrt <- sample_sd / sqrt(sample_size)
  head <- sample_mean - (1.96 * sd_sqrt)
  tail <- sample_mean + (1.96 * sd_sqrt)
  return(c(head, tail))
}

mean_w <- mean(dur_word_freq$LogDurationW)
sd_w <- sd(dur_word_freq$LogDurationW)
n_w <- length(dur_word_freq$LogDurationW)
ci_(mean_w, sd_w, n_w)
```


#### 3.2 Функция `t.test`

Примените функцию `t.test` к той же переменной, чтобы узнать доверительный интервал среднего. 

```{r ci-ttest}
t.test(dur_word_freq$LogDurationW)
```

Совпадают ли результаты вычислений в 3.1 и 3.2?
```
Ответ: не строго совпадают, но разница между ними минимальная (менее 0.0001 <-> менее одной 10 000 тысячной)
```


#### 3.3 Другие уровни доверия  

С помощью функции `MeanCI` из пакета `DescTools` найдите 99% доверительный интервал для той же переменнной. Он шире или уже, чем 95% CI?  

Подсказка: используйте аргумент `conf.level`. 

```{r}
# install.packages('DescTools')
library(DescTools)
```


```{r ci-99}
DescTools::MeanCI(dur_word_freq$LogDurationW,
                  conf.level = .99)
# такой интервал оказался ШИРЕ, чем 95-ый
```


## 4. В каком возрасте дети усваивают слова? 

#### Практика в Tydyverse

Data: https://www.kaggle.com/rtatman/when-do-children-learn-words?select=main_data.csv

Основная таблица включает информацию о 732 словах норвежского языка и возрасте их усвоения детьми (age of acquisition). Другая таблица включает дополнительную информацию о частотности слов по данным веб-корпуса (Norwegian Web as Corpus) и данным разговоров взрослых с детьми.

Релевантные переменные (main data):

**Translation**: английский перевод норвежского слова

**AoA**: возраст усвоения в месяцах (в среднем, в каком возрасте слово усваивается детьми)

**VSoA**: сколько других слов знает "обобщенный" ребенок к моменту усвоения данного слова (округленно к ближайшему десятку)

**Broad_lex**: часть речи данного слова (в широком понимании) 

**CDS_Freq**: частота, с которой взрослый-носитель норвежского языка обращается к ребенку

Релевантные переменные (Norwegian CDS Frequency):

**Translation**: английский перевод норвежского слова

**Freq_NoWaC**: частота по данным корпуса NoWaC (интернет-коммуникация)

**Freq_CDS**: частота слова (по материалам двух норвежских корпусов CHILDES) при обращении к ребенку

NB! Все остальные переменные должны быть удалены из данных.  

NB! Столбцы 'Freq_CDS' and 'CDS_Freq' - это одно и то же. 

#### Данные 

#### 4.1 
Прочитайте обе таблицы   

```{r read both tables}
setwd("C:/Users/user/Downloads")
main_data <- read_csv('main_data.csv')
norv_freq <- read_csv('Norwegian_CDS_frequency.csv')
main_data
norv_freq
```



#### 4.2 
Оставьте только необходимые для последующего исследования переменные.

```{r clearing df}

main_data_clear <- main_data %>%
  select(Translation, AoA, VSoA, Broad_lex, CDS_freq)
main_data_clear

norv_freq_clear <- norv_freq %>%
  select(Translation, Freq_NoWaC, Freq_CDS)
norv_freq_clear

```

#### 4.3  
Объедините две таблицы в общий датафрейм/тиббл под названием 'norw_words'. 

NB! В вашем датафрейме не должно быть дублирования столбцов. 

```{r inner-join}
norw_words <- main_data_clear %>%
  inner_join(norv_freq_clear)
norw_words
```

#### 4.4  
Оставьте только 15 первых строк 
 
```{r}
norw_words_15_rows <- norw_words %>%
                      slice(1:15)
norw_words_15_rows
```


#### 5. Преобразования данных 

#### 5.1  
Создайте тиббл 'freq_statistics', используя 3 столбца из (полного) тиббла `norw_words`: 'Translation', 'CDS_Freq', 'Freq_NoWaC'. Затем с помощью функций `pivot_longer`/`pivot_wider` измените формат тиббла с "широкого" на "узкий" или наоборот (данные подскажут вам, с какого на какой менять).

```{r}
freq_statistics <- norw_words %>%
  select(Translation, CDS_freq, Freq_NoWaC)

freq_statistics %>%
  pivot_longer(cols=CDS_freq:Freq_NoWaC,
               names_to='freq',
               values_to='stats'
              )

```

#### 5.2  
Получите строковый вектор (string vector output) с информацией о классах переменных в тиббле. 

```{r}
freq_statistics %>%
  map(class)
```

#### 5.3  
Представьте ту же информацию как датафрейм (dataframe). 

```{r}
freq_statistics %>%
  map_df(class)
```

#### 5.4  
Конвертируйте значения из столбцов `CDS_Freq` и `Freq_NoWaC` тиббла `freq_statistics` в числовые (numeric). 

```{r}
freq_statistics %>%
  mutate(across(CDS_freq:Freq_NoWaC,
                as.integer)
         )
```

#### 5.5 
Получите средние значения всех числовых типов переменных в `norw_words`.

```{r}
norw_words <- norw_words %>%
              mutate(across(c('AoA', 'VSoA', 'CDS_freq', 'Freq_NoWaC', 'Freq_CDS'),
                            as.integer))
norw_words %>%
  select(where(is.numeric)) %>%
  map_dbl(mean, na.rm=TRUE)
```

#### 5.6 
Создайте вложенную структуру (nested table) по столбцу `Translation`.
 
```{r}
norw_words %>%
  nest(!Translation)
```

#### 6.1 Корреляционный анализ  

Из доступных данных, выберите несколько переменных (более двух) для проведения корреляционного анализа, визуализируйте тепловую карту или другой тип коррелограммы. 


```{r}
# install.packages('corrplot')
# install.packages("psych")
library(psych)
library(corrplot)
```


```{r}
norw_words_cor_matrix <- 
  norw_words %>%
  select(where(is.numeric)) %>%
  drop_na() %>%
  corr.test()

corrplot(
  norw_words_cor_matrix$r,
  p.mat=norw_words_cor_matrix$p,
  insig = "p-value"
  )
```



#### 6.2 
Выводы  

В поле ниже сформулируйте ваши выводы, приведите p-values и другие необходимые статистические метрики. 

```
На корреляционной матрице мы присвоили p-value парам, корреляция между которыми — незначительна.
Если мы глянем на p-value значительных значений, то увидем, что они значительно ниже нуля или 0, а у незначительных — явно выше 0.05. Таким образом p-value подтверждает корреляционный анализ, который в свою очередь позволяет сделать нам следующие выводы:
1) Все числовые переменные можно разделить на две группы
  A) AoA, VsoA
  B) CDS_freq, Freq_NoWaC, Freq_CDS
2) Члены одной группы имеют сильну корреляцию (и крайне ниже нуля p-value) МЕЖДУ СОБОЙ, и крайне слабую корреляцию (высокий p-value) МЕЖДУ ЧЛЕНАМИ РАЗНЫХ ГРУПП


```

```{r}
norw_words_cor_matrix$p
```

```{r}
norw_words_cor_matrix$r
```


